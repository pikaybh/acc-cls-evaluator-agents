from prompts import user_query_v3, evaluator_prompt_v3
from models import gpt_call, ollama_call


def loop_workflow_v1(user_query, evaluator_prompt, max_retries=5, logger=None) -> str:
    """í‰ê°€ìê°€ ìƒì„±ëœ ìš”ì•½ì„ í†µê³¼í•  ë•Œê¹Œì§€ ìµœëŒ€ max_retriesë²ˆ ë°˜ë³µ."""
    if logger is None:
        raise ValueError("logger must be provided from main.py")

    retries = 0
    while retries < max_retries:
        logger.debug(f"ğŸ“ ì‚¬ê³  ìœ í˜• ë¶„ë¥˜ í”„ë¡¬í”„íŠ¸ (ì‹œë„ {retries + 1}/{max_retries})\n{user_query}\n")
        
        labels = gpt_call(user_query, model="gpt-4.1-mini")
        logger.debug(f"ğŸ“ ì‚¬ê³  ìœ í˜• ë¶„ë¥˜ ê²°ê³¼ (ì‹œë„ {retries + 1}/{max_retries})\nì‚¬ê³  ìœ í˜•: {labels}\n")
        
        final_evaluator_prompt = evaluator_prompt + labels
        evaluation_result = gpt_call(final_evaluator_prompt, model="gpt-4.1").strip()
        logger.debug(f"ğŸ” í‰ê°€ í”„ë¡¬í”„íŠ¸ (ì‹œë„ {retries + 1}/{max_retries})\n{final_evaluator_prompt}\n")
        logger.debug(f"ğŸ” í‰ê°€ ê²°ê³¼ (ì‹œë„ {retries + 1}/{max_retries})\n{evaluation_result}\n")

        if "í‰ê°€ê²°ê³¼ = PASS" in evaluation_result:
            logger.debug("âœ… í†µê³¼! ìµœì¢… ì‚¬ê³  ìœ í˜• ë¶„ë¥˜ê°€ ìŠ¹ì¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return labels
        
        retries += 1
        logger.debug(f"ğŸ”„ ì¬ì‹œë„ í•„ìš”... ({retries}/{max_retries})")

        # If max retries reached, return last attempt
        if retries >= max_retries:
            logger.debug("âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ë„ë‹¬. ë§ˆì§€ë§‰ ë¶„ë¥˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
            # ìµœì¢… ì‹œë„ì— ëŒ€í•´ í‰ê°€ LLMì„ í•œ ë²ˆ ë” í˜¸ì¶œ
            final_eval = gpt_call(final_evaluator_prompt + labels, model="gpt-4.1").strip()
            if "í‰ê°€ê²°ê³¼ = PASS" in final_eval:
                return labels
            else:
                return "ê¸°íƒ€"

        # Updating the user_query for the next attempt with full history
        user_query += f"\n{retries}ì°¨ ì‚¬ê³  ìœ í˜• ë¶„ë¥˜ ê²°ê³¼: {labels}\n"
        user_query += f"\n{retries}ì°¨ ì‚¬ê³  ìœ í˜• ë¶„ë¥˜ í”¼ë“œë°±:\n\n{evaluation_result}\n\n"


def loop_workflow_v2(user_query, evaluator_prompt, max_retries=5, logger=None) -> str:
    """í‰ê°€ìê°€ ìƒì„±ëœ ìš”ì•½ì„ í†µê³¼í•  ë•Œê¹Œì§€ ìµœëŒ€ max_retriesë²ˆ ë°˜ë³µ."""
    if logger is None:
        raise ValueError("logger must be provided from main.py")

    retries = 0
    while retries < max_retries:
        # Prompting the user query
        logger.debug(f"ğŸ“ ì‚¬ê³  ìœ í˜• ë¶„ë¥˜ í”„ë¡¬í”„íŠ¸ (ì‹œë„ {retries + 1}/{max_retries})\n{user_query}\n")
        
        # Call the LLM to classify the accident type
        labels = gpt_call(user_query, model="gpt-4.1-mini").strip()
        logger.debug(f"ğŸ“ ì‚¬ê³  ìœ í˜• ë¶„ë¥˜ ê²°ê³¼ (ì‹œë„ {retries + 1}/{max_retries})\nì‚¬ê³  ìœ í˜•: {labels}\n")
        
        # Call Evaluator LLM to evaluate the classification
        final_evaluator_prompt = evaluator_prompt + labels
        evaluation_result = gpt_call(final_evaluator_prompt, model="gpt-4.1").strip()
        logger.debug(f"ğŸ” í‰ê°€ í”„ë¡¬í”„íŠ¸ (ì‹œë„ {retries + 1}/{max_retries})\n{final_evaluator_prompt}\n")
        logger.debug(f"ğŸ” í‰ê°€ ê²°ê³¼ (ì‹œë„ {retries + 1}/{max_retries})\n{evaluation_result}\n")

        if "í‰ê°€ê²°ê³¼ = PASS" in evaluation_result:
            logger.debug("âœ…âœ…âœ… í†µê³¼! ìµœì¢… ì‚¬ê³  ìœ í˜• ë¶„ë¥˜ê°€ ìŠ¹ì¸ë˜ì—ˆìŠµë‹ˆë‹¤. âœ…âœ…âœ…")
            return labels
        
        retries += 1
        logger.debug(f"ğŸ”„ ì¬ì‹œë„ í•„ìš”... ({retries}/{max_retries})")

        # If max retries reached, return last attempt
        if retries >= max_retries:
            logger.debug("âŒâŒâŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ë„ë‹¬. ë§ˆì§€ë§‰ ë¶„ë¥˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. âŒâŒâŒ")
            return labels

        # Updating the user_query for the next attempt with full history
        user_query += f"\n{retries}ì°¨ ì‚¬ê³  ìœ í˜• ë¶„ë¥˜ ê²°ê³¼: {labels}\n"
        user_query += f"\n{retries}ì°¨ ì‚¬ê³  ìœ í˜• ë¶„ë¥˜ í”¼ë“œë°±:\n\n{evaluation_result}\n\n"


def loop_workflow_v3(user_query, evaluator_prompt, max_retries=5, logger=None) -> str:
    """í‰ê°€ìê°€ ìƒì„±ëœ ìš”ì•½ì„ í†µê³¼í•  ë•Œê¹Œì§€ ìµœëŒ€ max_retriesë²ˆ ë°˜ë³µ."""
    if logger is None:
        raise ValueError("logger must be provided from main.py")

    retries = 0
    while retries < max_retries:
        # Prompting the user query
        logger.debug(f"ğŸ“ ì‚¬ê³  ìœ í˜• ë¶„ë¥˜ í”„ë¡¬í”„íŠ¸ (ì‹œë„ {retries + 1}/{max_retries})\n{user_query}\n")
        
        # Call the LLM to classify the accident type
        labels = gpt_call(user_query, model="gpt-4.1-mini").strip()
        logger.debug(f"ğŸ“ ì‚¬ê³  ìœ í˜• ë¶„ë¥˜ ê²°ê³¼ (ì‹œë„ {retries + 1}/{max_retries})\nì‚¬ê³  ìœ í˜•: {labels}\n")
        
        # Call Evaluator LLM to evaluate the classification
        final_evaluator_prompt = evaluator_prompt + labels
        evaluation_result = gpt_call(final_evaluator_prompt, model="gpt-4.1").strip()
        logger.debug(f"ğŸ” í‰ê°€ í”„ë¡¬í”„íŠ¸ (ì‹œë„ {retries + 1}/{max_retries})\n{final_evaluator_prompt}\n")
        logger.debug(f"ğŸ” í‰ê°€ ê²°ê³¼ (ì‹œë„ {retries + 1}/{max_retries})\n{evaluation_result}\n")

        if "í‰ê°€ê²°ê³¼ = PASS" in evaluation_result:
            logger.debug("âœ…âœ…âœ… í†µê³¼! ìµœì¢… ì‚¬ê³  ìœ í˜• ë¶„ë¥˜ê°€ ìŠ¹ì¸ë˜ì—ˆìŠµë‹ˆë‹¤. âœ…âœ…âœ…")
            return labels
        
        retries += 1
        logger.debug(f"ğŸ”„ ì¬ì‹œë„ í•„ìš”... ({retries}/{max_retries})")

        # If max retries reached, return last attempt
        if retries >= max_retries:
            logger.debug("âŒâŒâŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ë„ë‹¬. ë§ˆì§€ë§‰ ë¶„ë¥˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. âŒâŒâŒ")
            return labels

        # Updating the user_query for the next attempt with full history
        user_query += f"\n{retries}ì°¨ ì‚¬ê³  ìœ í˜• ë¶„ë¥˜ ê²°ê³¼: {labels}\n"
        user_query += f"\n{retries}ì°¨ ì‚¬ê³  ìœ í˜• ë¶„ë¥˜ í”¼ë“œë°±:\n\n{evaluation_result}\n\n"


def invoke_chain(input_content, max_retries, logger=None):
    if logger is None:
        raise ValueError("logger must be provided from main.py")
    final_labels = loop_workflow_v3(user_query_v3.format(input_content), evaluator_prompt_v3, max_retries=max_retries, logger=logger)
    logger.debug(f"ğŸ’¡ğŸ’¡ğŸ’¡ ìµœì¢… ê²°ê³¼ ğŸ’¡ğŸ’¡ğŸ’¡\nìœ„í—˜ì„± í‰ê°€:\n{input_content}\nì‚¬ê³  ìœ í˜• ë¶„ë¥˜: {final_labels}\n")
    return final_labels


__all__ = ["invoke_chain"]